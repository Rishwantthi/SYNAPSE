{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSn3MBo61Yg9/WMPlc3z/1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishwantthi/SYNAPSE/blob/main/Imagetxtaudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install transformers accelerate bitsandbytes gTTS gradio --quiet\n",
        "!apt-get install -y tesseract-ocr --quiet\n",
        "!pip install pytesseract opencv-python --quiet\n",
        "\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import torch\n",
        "from gtts import gTTS\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load BLIP model (single transformer: ViT + BERT)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "# --- OCR Preprocessing ---\n",
        "def preprocess_for_ocr(pil_image):\n",
        "    img = np.array(pil_image)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
        "    img = cv2.GaussianBlur(img, (5,5), 0)\n",
        "    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    return img\n",
        "\n",
        "def extract_ocr_text(pil_image):\n",
        "    preprocessed_img = preprocess_for_ocr(pil_image)\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "    text = pytesseract.image_to_string(preprocessed_img, config=custom_config)\n",
        "    return text.strip()\n",
        "\n",
        "# --- Main Pipeline ---\n",
        "def process_image(image):\n",
        "    # 1. Generate Caption\n",
        "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    # 2. Extract OCR (for text display only)\n",
        "    ocr_text = extract_ocr_text(image)\n",
        "\n",
        "    # 3. Convert Caption to Speech (OCR NOT included)\n",
        "    tts = gTTS(caption)\n",
        "    audio_path = \"caption_audio.mp3\"\n",
        "    tts.save(audio_path)\n",
        "\n",
        "    return caption, ocr_text, audio_path\n",
        "\n",
        "# --- Gradio UI ---\n",
        "demo = gr.Interface(\n",
        "    fn=process_image,\n",
        "    inputs=gr.Image(type=\"pil\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Image Caption (BLIP)\"),\n",
        "        gr.Textbox(label=\"OCR Extracted Text\"),\n",
        "        gr.Audio(label=\"Caption Audio Output\")\n",
        "    ],\n",
        "    title=\"ImageSpeak++ (Hackathon Edition)\",\n",
        "    description=\"Upload an image â†’ BLIP caption + OCR text (display only) + audio narration of caption.\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "EuueVJcaWC6f",
        "outputId": "297d6ad7-1f1f-4f82-cc09-0609049f067e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://249a9e38d42776815a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://249a9e38d42776815a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`cache.key_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].keys` instead.\n",
            "`cache.value_cache[idx]` is deprecated and will be removed in v4.56.0. Use `cache.layers[idx].values` instead.\n"
          ]
        }
      ]
    }
  ]
}